DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/dataset_infos.json HTTP/1.1" 200 0
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
WARNING:datasets.fingerprint:Parameter 'function'=<function Director.transform at 0x7f9b50a0b700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
INFO:root:Created new esnli deepa2 dataset: DatasetDict({
    train: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 31200
    })
    validation: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 26144
    })
    test: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 24992
    })
})
INFO:root:###################################################
INFO:root:Starting new esnli transformation: 1643019018.688769
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/dataset_infos.json HTTP/1.1" 200 0
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
INFO:root:###################################################
INFO:root:Starting new esnli transformation: 1643019066.469101
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/dataset_infos.json HTTP/1.1" 200 0
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
WARNING:root:flawed esnli batch with different premises ['2 little girls made paper plate snowmen.', '2 little girls, both wearing life jackets, run along a shoreline scaring seagulls', '2 little girls, both wearing life jackets, run along a shoreline scaring seagulls'].
WARNING:root:flawed esnli batch with different premises ['2 swimmers in mid flight doing a dive into a pool', '2 swimmers in mid flight doing a dive into a pool', '2 teams of men playing football.'].
WARNING:root:flawed esnli batch with labels [0, 2, 0].
WARNING:root:flawed esnli batch with labels [1, 2, 1].
WARNING:root:flawed esnli batch with different premises ['A Caucasian male jumping behind a graffited wall.', 'A Caucasian male musician playing a xylophone.', 'A Caucasian male musician playing a xylophone.'].
WARNING:root:flawed esnli batch with different premises ['4 people stand outside a photography store', '4 teenagers are standing outside a Wells Fargo bank.', '4 teenagers are standing outside a Wells Fargo bank.'].
WARNING:root:flawed esnli batch with different premises ['2 men running in warm clothing, while another man is biking on a pier in the stormy looking weather.', '2 men running in warm clothing, while another man is biking on a pier in the stormy looking weather.', '2 men stand in line at a restaurant.'].
WARNING:root:flawed esnli batch with batch size 2.
INFO:root:###################################################
INFO:root:Starting new esnli transformation: 1643019127.958124
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/dataset_infos.json HTTP/1.1" 200 0
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
WARNING:datasets.fingerprint:Parameter 'function'=<function Director.transform at 0x7f9000384670> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
INFO:root:Created new esnli deepa2 dataset: DatasetDict({
    train: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 31200
    })
    validation: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 26144
    })
    test: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 24992
    })
})
INFO:root:Removed metadata from deepa2 dataset
INFO:root:Saved esnli deepa2 dataset to data/esnlii/esnli.
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-24 11:40:13.278509
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/dataset_infos.json HTTP/1.1" 200 0
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
WARNING:datasets.fingerprint:Parameter 'function'=<function Director.transform at 0x7fe9d0b03670> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
INFO:root:Created new esnli deepa2 dataset: DatasetDict({
    train: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 31200
    })
    validation: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 26144
    })
    test: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 24992
    })
})
INFO:root:Removed metadata from deepa2 dataset
INFO:root:Saving esnli split train ...
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-24 11:45:27.624232
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/dataset_infos.json HTTP/1.1" 200 0
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
WARNING:datasets.fingerprint:Parameter 'function'=<function Director.transform at 0x7fd5916d3670> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
INFO:root:Created new esnli deepa2 dataset: DatasetDict({
    train: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 31200
    })
    validation: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 26144
    })
    test: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 24992
    })
})
INFO:root:Removed metadata from deepa2 dataset
INFO:root:Saving esnli split train ...
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-24 11:57:52.782501
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/dataset_infos.json HTTP/1.1" 200 0
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
WARNING:datasets.fingerprint:Parameter 'function'=<function Director.transform at 0x7fdaa1243670> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
INFO:root:Created new esnli deepa2 dataset: DatasetDict({
    train: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 31200
    })
    validation: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 26144
    })
    test: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 24992
    })
})
INFO:root:Removed metadata from deepa2 dataset
INFO:root:Saving esnli split train ...
INFO:root:Saving esnli split validation ...
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-24 12:01:35.831120
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/dataset_infos.json HTTP/1.1" 200 0
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
WARNING:datasets.fingerprint:Parameter 'function'=<function Director.transform at 0x7f94d8974a60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
INFO:root:Created new esnli deepa2 dataset: DatasetDict({
    train: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 31200
    })
    validation: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 26144
    })
    test: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'id', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 24992
    })
})
INFO:root:Removed metadata from deepa2 dataset
INFO:root:Saving esnli split train ...
INFO:root:Saving esnli split validation ...
INFO:root:Saving esnli split test ...
INFO:root:Saved esnli deepa2 dataset to data/esnli.
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-24 12:24:36.554642
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/dataset_infos.json HTTP/1.1" 200 0
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-24 12:25:40.521670
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/dataset_infos.json HTTP/1.1" 200 0
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
WARNING:datasets.fingerprint:Parameter 'function'=<function Director.transform at 0x7fd6a85c4d30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
INFO:root:Created new esnli deepa2 dataset: DatasetDict({
    train: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 31200
    })
    validation: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 26144
    })
    test: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 24992
    })
})
INFO:root:Saving esnli split train ...
INFO:root:Saving esnli split validation ...
INFO:root:Saving esnli split test ...
INFO:root:Saved esnli deepa2 dataset to data/esnli.
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-24 12:34:49.882294
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.17.0/datasets/esnli/dataset_infos.json HTTP/1.1" 200 0
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
WARNING:datasets.fingerprint:Parameter 'function'=<function Director.transform at 0x7f8da893b670> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
INFO:root:Created new esnli deepa2 dataset: DatasetDict({
    train: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 31200
    })
    validation: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 26144
    })
    test: Dataset({
        features: ['argdown_reconstruction', 'argument_source', 'conclusion', 'conclusion_formalized', 'conclusion_statements', 'context', 'distractors', 'entity_placeholders', 'erroneous_argdown', 'gist', 'intermediary_conclusion', 'intermediary_conclusion_formalized', 'metadata', 'misc_placeholders', 'predicate_placeholders', 'premises', 'premises_formalized', 'reason_statements', 'source_paraphrase', 'title'],
        num_rows: 24992
    })
})
INFO:root:Saving esnli split train ...
INFO:root:Saving esnli split validation ...
INFO:root:Saving esnli split test ...
INFO:root:Saved esnli deepa2 dataset to data/processed/esnli.
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-25 09:06:14.896248
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
WARNING:datasets.fingerprint:Parameter 'function'=<function Director.transform at 0x7f8fc046ee50> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
INFO:root:Created new esnli deepa2 dataset: DatasetDict({
    train: Dataset({
        features: ['argument_source', 'title', 'gist', 'source_paraphrase', 'context', 'argdown_reconstruction', 'erroneous_argdown', 'reason_statements', 'conclusion_statements', 'premises', 'intermediary_conclusion', 'conclusion', 'premises_formalized', 'intermediary_conclusion_formalized', 'conclusion_formalized', 'predicate_placeholders', 'entity_placeholders', 'misc_placeholders', 'distractors', 'metadata'],
        num_rows: 31200
    })
    validation: Dataset({
        features: ['argument_source', 'title', 'gist', 'source_paraphrase', 'context', 'argdown_reconstruction', 'erroneous_argdown', 'reason_statements', 'conclusion_statements', 'premises', 'intermediary_conclusion', 'conclusion', 'premises_formalized', 'intermediary_conclusion_formalized', 'conclusion_formalized', 'predicate_placeholders', 'entity_placeholders', 'misc_placeholders', 'distractors', 'metadata'],
        num_rows: 26144
    })
    test: Dataset({
        features: ['argument_source', 'title', 'gist', 'source_paraphrase', 'context', 'argdown_reconstruction', 'erroneous_argdown', 'reason_statements', 'conclusion_statements', 'premises', 'intermediary_conclusion', 'conclusion', 'premises_formalized', 'intermediary_conclusion_formalized', 'conclusion_formalized', 'predicate_placeholders', 'entity_placeholders', 'misc_placeholders', 'distractors', 'metadata'],
        num_rows: 24992
    })
})
INFO:root:Saving esnli split train ...
INFO:root:Saving esnli split validation ...
INFO:root:Saving esnli split test ...
INFO:root:Saved esnli deepa2 dataset to data/processed/esnli.
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-25 09:22:41.446169
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
WARNING:datasets.fingerprint:Parameter 'function'=<function Director.transform at 0x7fb66922f790> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
INFO:root:Created new esnli deepa2 dataset: DatasetDict({
    train: Dataset({
        features: ['argument_source', 'title', 'gist', 'source_paraphrase', 'context', 'argdown_reconstruction', 'erroneous_argdown', 'reason_statements', 'conclusion_statements', 'premises', 'intermediary_conclusion', 'conclusion', 'premises_formalized', 'intermediary_conclusion_formalized', 'conclusion_formalized', 'predicate_placeholders', 'entity_placeholders', 'misc_placeholders', 'distractors', 'metadata'],
        num_rows: 31200
    })
    validation: Dataset({
        features: ['argument_source', 'title', 'gist', 'source_paraphrase', 'context', 'argdown_reconstruction', 'erroneous_argdown', 'reason_statements', 'conclusion_statements', 'premises', 'intermediary_conclusion', 'conclusion', 'premises_formalized', 'intermediary_conclusion_formalized', 'conclusion_formalized', 'predicate_placeholders', 'entity_placeholders', 'misc_placeholders', 'distractors', 'metadata'],
        num_rows: 26144
    })
    test: Dataset({
        features: ['argument_source', 'title', 'gist', 'source_paraphrase', 'context', 'argdown_reconstruction', 'erroneous_argdown', 'reason_statements', 'conclusion_statements', 'premises', 'intermediary_conclusion', 'conclusion', 'premises_formalized', 'intermediary_conclusion_formalized', 'conclusion_formalized', 'predicate_placeholders', 'entity_placeholders', 'misc_placeholders', 'distractors', 'metadata'],
        num_rows: 24992
    })
})
INFO:root:Saving esnli split train ...
INFO:root:Saving esnli split validation ...
INFO:root:Saving esnli split test ...
INFO:root:Saved esnli deepa2 dataset to data/processed/esnli.
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-25 09:26:52.164434
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
WARNING:datasets.fingerprint:Parameter 'function'=<function Director.transform at 0x7f9aa0c858b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
INFO:root:Created new esnli deepa2 dataset: DatasetDict({
    train: Dataset({
        features: ['argument_source', 'title', 'gist', 'source_paraphrase', 'context', 'argdown_reconstruction', 'erroneous_argdown', 'reason_statements', 'conclusion_statements', 'premises', 'intermediary_conclusion', 'conclusion', 'premises_formalized', 'intermediary_conclusion_formalized', 'conclusion_formalized', 'predicate_placeholders', 'entity_placeholders', 'misc_placeholders', 'distractors', 'metadata'],
        num_rows: 31200
    })
    validation: Dataset({
        features: ['argument_source', 'title', 'gist', 'source_paraphrase', 'context', 'argdown_reconstruction', 'erroneous_argdown', 'reason_statements', 'conclusion_statements', 'premises', 'intermediary_conclusion', 'conclusion', 'premises_formalized', 'intermediary_conclusion_formalized', 'conclusion_formalized', 'predicate_placeholders', 'entity_placeholders', 'misc_placeholders', 'distractors', 'metadata'],
        num_rows: 26144
    })
    test: Dataset({
        features: ['argument_source', 'title', 'gist', 'source_paraphrase', 'context', 'argdown_reconstruction', 'erroneous_argdown', 'reason_statements', 'conclusion_statements', 'premises', 'intermediary_conclusion', 'conclusion', 'premises_formalized', 'intermediary_conclusion_formalized', 'conclusion_formalized', 'predicate_placeholders', 'entity_placeholders', 'misc_placeholders', 'distractors', 'metadata'],
        num_rows: 24992
    })
})
INFO:root:Saving esnli split train ...
INFO:root:Saving esnli split validation ...
INFO:root:Saving esnli split test ...
INFO:root:Saved esnli deepa2 dataset to data/processed/esnli.
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-25 10:03:44.250568
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-25 10:04:32.850371
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-25 10:06:48.656619
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-25 10:08:05.011895
DEBUG:root:['{{ premise | lower }}', '{{ premise | conditional(hypothesis) }}', '{{ hypothesis | lower }}']
DEBUG:root:['{ premise | lower }', '{ premise | conditional(hypothesis) }', '{ hypothesis | lower }']
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-25 10:10:00.871317
DEBUG:root:['{{ premise | lower }}', '{{ premise | conditional(hypothesis) }}', '{{ hypothesis | lower }}']
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-25 10:14:16.739520
DEBUG:root:['{{ premise | lower }}', '{{ premise | conditional(hypothesis) }}', '{{ hypothesis | lower }}']
DEBUG:root:['{{ premise | lower }}', '{{ premise | conditional(hypothesis) }}', '{{ hypothesis | lower }}']
DEBUG:root:['{{ premise | lower }}', '{{ premise | conditional(hypothesis | negation) }}', '{{ hypothesis | negation }}']
DEBUG:root:['{{ premise | lower }}', '{{ premise | conditional(hypothesis | negation ) }}', '{{ hypothesis | negation }}']
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-25 10:14:55.096989
DEBUG:root:['{{ premise | lower }}', '{{ premise | conditional(hypothesis) }}', '{{ hypothesis | lower }}']
DEBUG:root:['{{ premise | lower }}', '{{ premise | conditional(hypothesis) }}', '{{ hypothesis | lower }}']
DEBUG:root:['{{ premise | lower }}', '{{ premise | conditional(hypothesis | negation) }}', '{{ hypothesis | negation }}']
DEBUG:root:['{{ premise | lower }}', '{{ premise | conditional(hypothesis | negation) }}', '{{ hypothesis | negation }}']
DEBUG:root:['{{ hypothesis | lower }}', '{{ hypothesis | conditional(premise | negation) }}', '{{ premise | negation }}']
DEBUG:root:['{{ hypothesis | lower }}', '{{ hypothesis | conditional(premise | negation) }}', '{{ premise | negation }}']
DEBUG:root:['{{ premise | lower }}', '{{ hypothesis | conditional(premise | negation) }}', '{{ hypothesis | negation }}']
DEBUG:root:['{{ premise | lower }}', '{{ hypothesis | conditional(premise | negation) }}', '{{ hypothesis | negation }}']
DEBUG:root:['{{ hypothesis | lower }}', '{{ premise | conditional(hypothesis | negation) }}', '{{ premise | negation }}']
DEBUG:root:['{{ hypothesis | lower }}', '{{ premise | conditional(hypothesis | negation) }}', '{{ premise | negation }}']
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.18.0/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.18.0/datasets/esnli/dataset_infos.json HTTP/1.1" 200 0
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-25 10:15:23.338696
DEBUG:root:['{{ premise | lower }}', '{{ premise | conditional(hypothesis) }}', '{{ hypothesis | lower }}']
DEBUG:root:['{{ premise | lower }}', '{{ premise | conditional(hypothesis) }}', '{{ hypothesis | lower }}']
DEBUG:root:['{{ premise | lower }}', '{{ premise | conditional(hypothesis | negation) }}', '{{ hypothesis | negation }}']
DEBUG:root:['{{ premise | lower }}', '{{ premise | conditional(hypothesis | negation) }}', '{{ hypothesis | negation }}']
DEBUG:root:['{{ hypothesis | lower }}', '{{ hypothesis | conditional(premise | negation) }}', '{{ premise | negation }}']
DEBUG:root:['{{ hypothesis | lower }}', '{{ hypothesis | conditional(premise | negation) }}', '{{ premise | negation }}']
DEBUG:root:['{{ premise | lower }}', '{{ hypothesis | conditional(premise | negation) }}', '{{ hypothesis | negation }}']
DEBUG:root:['{{ premise | lower }}', '{{ hypothesis | conditional(premise | negation) }}', '{{ hypothesis | negation }}']
DEBUG:root:['{{ hypothesis | lower }}', '{{ premise | conditional(hypothesis | negation) }}', '{{ premise | negation }}']
DEBUG:root:['{{ hypothesis | lower }}', '{{ premise | conditional(hypothesis | negation) }}', '{{ premise | negation }}']
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.18.0/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.18.0/datasets/esnli/dataset_infos.json HTTP/1.1" 200 0
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-25 10:16:35.180372
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.18.0/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.18.0/datasets/esnli/dataset_infos.json HTTP/1.1" 200 0
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
WARNING:datasets.fingerprint:Parameter 'function'=<function Director.transform at 0x7ff350cc78b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
INFO:root:Created new esnli deepa2 dataset: DatasetDict({
    train: Dataset({
        features: ['argument_source', 'title', 'gist', 'source_paraphrase', 'context', 'argdown_reconstruction', 'erroneous_argdown', 'reason_statements', 'conclusion_statements', 'premises', 'intermediary_conclusion', 'conclusion', 'premises_formalized', 'intermediary_conclusion_formalized', 'conclusion_formalized', 'predicate_placeholders', 'entity_placeholders', 'misc_placeholders', 'distractors', 'metadata'],
        num_rows: 31200
    })
    validation: Dataset({
        features: ['argument_source', 'title', 'gist', 'source_paraphrase', 'context', 'argdown_reconstruction', 'erroneous_argdown', 'reason_statements', 'conclusion_statements', 'premises', 'intermediary_conclusion', 'conclusion', 'premises_formalized', 'intermediary_conclusion_formalized', 'conclusion_formalized', 'predicate_placeholders', 'entity_placeholders', 'misc_placeholders', 'distractors', 'metadata'],
        num_rows: 26144
    })
    test: Dataset({
        features: ['argument_source', 'title', 'gist', 'source_paraphrase', 'context', 'argdown_reconstruction', 'erroneous_argdown', 'reason_statements', 'conclusion_statements', 'premises', 'intermediary_conclusion', 'conclusion', 'premises_formalized', 'intermediary_conclusion_formalized', 'conclusion_formalized', 'predicate_placeholders', 'entity_placeholders', 'misc_placeholders', 'distractors', 'metadata'],
        num_rows: 24992
    })
})
INFO:root:Saving esnli split train ...
INFO:root:Saving esnli split validation ...
INFO:root:Saving esnli split test ...
INFO:root:Saved esnli deepa2 dataset to data/processed/esnli.
INFO:root:#################################################################
INFO:root:Starting new esnli transformation: 2022-01-25 10:32:45.525863
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443
DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 "HEAD /datasets.huggingface.co/datasets/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.18.0/datasets/esnli/esnli.py HTTP/1.1" 200 0
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): raw.githubusercontent.com:443
DEBUG:urllib3.connectionpool:https://raw.githubusercontent.com:443 "HEAD /huggingface/datasets/1.18.0/datasets/esnli/dataset_infos.json HTTP/1.1" 200 0
WARNING:datasets.builder:Reusing dataset esnli (/Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)
INFO:root:Loaded esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 549367
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9842
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 9824
    })
})
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-0b5a98c1dadf8f32.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-edf41699dbb34eaf.arrow
WARNING:datasets.arrow_dataset:Loading cached sorted indices for dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-f7d4f60a740881d1.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-4a801acb70ebe9d5.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-18e2c571f18ea5c7.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/ggbetz/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc/cache-d8a5fba68e2d04c0.arrow
INFO:root:Debug mode, working with filtered dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 3000
    })
})
INFO:root:Preprocessing esnli split train ...
INFO:root:Preprocessing esnli split validation ...
INFO:root:Preprocessing esnli split test ...
INFO:root:Preprocessed esnli dataset: DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2925
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2451
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],
        num_rows: 2343
    })
})
WARNING:datasets.fingerprint:Parameter 'function'=<function Director.transform at 0x7ff780192f70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
INFO:root:Created new esnli deepa2 dataset: DatasetDict({
    train: Dataset({
        features: ['argument_source', 'title', 'gist', 'source_paraphrase', 'context', 'argdown_reconstruction', 'erroneous_argdown', 'reason_statements', 'conclusion_statements', 'premises', 'intermediary_conclusion', 'conclusion', 'premises_formalized', 'intermediary_conclusion_formalized', 'conclusion_formalized', 'predicate_placeholders', 'entity_placeholders', 'misc_placeholders', 'distractors', 'metadata'],
        num_rows: 31200
    })
    validation: Dataset({
        features: ['argument_source', 'title', 'gist', 'source_paraphrase', 'context', 'argdown_reconstruction', 'erroneous_argdown', 'reason_statements', 'conclusion_statements', 'premises', 'intermediary_conclusion', 'conclusion', 'premises_formalized', 'intermediary_conclusion_formalized', 'conclusion_formalized', 'predicate_placeholders', 'entity_placeholders', 'misc_placeholders', 'distractors', 'metadata'],
        num_rows: 26144
    })
    test: Dataset({
        features: ['argument_source', 'title', 'gist', 'source_paraphrase', 'context', 'argdown_reconstruction', 'erroneous_argdown', 'reason_statements', 'conclusion_statements', 'premises', 'intermediary_conclusion', 'conclusion', 'premises_formalized', 'intermediary_conclusion_formalized', 'conclusion_formalized', 'predicate_placeholders', 'entity_placeholders', 'misc_placeholders', 'distractors', 'metadata'],
        num_rows: 24992
    })
})
INFO:root:Saving esnli split train ...
INFO:root:Saving esnli split validation ...
INFO:root:Saving esnli split test ...
INFO:root:Saved esnli deepa2 dataset to data/processed/esnli.
